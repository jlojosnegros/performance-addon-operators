# Proposal: Performance Profile Controller Adaptation to Hypershift

## Additional information

### Hypershift docs

Documentation worth reading to get familiar with Hypershift.

- High level approach : <https://hypershift-docs.netlify.app/>
- Main concepts and actors: <https://hypershift-docs.netlify.app/reference/concepts-and-personas/>
- Architecture: <https://hypershift-docs.netlify.app/reference/controller-architecture/>

## Context

Performance Profile Controller optimizes OpenShift clusters for applications sensitive to cpu and network.
More info [here](https://github.com/openshift/cluster-node-tuning-operator/blob/master/docs/performanceprofile/performance_controller.md)

[PerformanceProfile CRD](https://github.com/openshift/cluster-node-tuning-operator/blob/master/docs/performanceprofile/performance_profile.md) is the API for Performance Profile Controller. Using a PerformanceProfile as input Performance Profile Controller currently generates four different objects:

- machineconfiguration.openshift.io/v1/MachineConfiguration
- machineconfiguration.openshift.io/v1/KubeletConfig
- Tuned
- RuntimeClass

In a Standalone deployment this CRs are generated by Performance Profile Controller and then reconciled by the appropriate controllers/operators running in the same cluster.
In a Hypershift deployment we need to change how these CRs are created.

__Note__: As Performance Profile Controller is part of [Cluster Node Tuning Operator](https://github.com/openshift/cluster-node-tuning-operator) (NTO) the proposal below will reference some of the NTO adaptations to Hypershift.

Here are the documents as reference:

- [Enhancement](https://github.com/openshift/enhancements/pull/1229)
- [NTO Phase one adaptation](https://github.com/openshift/cluster-node-tuning-operator/pull/390)
- [Hypershift changes for NTO adaptation](https://github.com/openshift/hypershift/pull/1651)
- [NTO Phase two adaptation proposal](https://github.com/openshift/hypershift/pull/1729)
- [NTO Hypershift adaptation design document](https://docs.google.com/document/d/1G9uu_EBCu-X8OgEA5L0P1GZMcrASjQJ6qIYAJSS6NbY/edit)

## Proposal

### Performance Profile

The proposal to handle the Performance Profile (PP) mimics the way NTO handles it `Tuned` API

- New `.spec.performanceProfileConfig` will be created in `NodePool` CRD
- [Cluster Service Consumer](https://hypershift-docs.netlify.app/reference/concepts-and-personas/) will create `PerformanceProfile` objects inside of configmaps, then reference these configmaps objects in the `.spec.performanceProfileConfig` field of the `NodePool` API
- NodePool Controller will read these configmaps, extract the `PerformanceProfile`s and propagate that information into one `configmap` per `NodePool` in the `hosted-control-plane-namespace` where Performance Profile Controller runs and can read them.
  - The `configmap` created by NodePool Controller will have the following labels and annotations:
    - label: `hypershift.openshift.io/performanceprofile-config` : `true`
    - label: `hypershift.openshift.io/nodePool` : `NodePool` API name where it was referenced.
    - annotation: `hypershift.openshift.io/nodePool` : `NodePool` API name where it was referenced.
- Performance Profile Controller will watch these configmaps and extract the `PerformanceProfile` objects from them to generate its output.

### Tuned

The proposal for this output object is to use the way NTO has already put in place to handle its own `tuned` objects as it is.

- Once Performance Profile Controller has created the `tuned` object as usual, it will embeded the `tuned` into a `configmap` in the `hosted-control-plane-namespace`.
  - This `configmap` will have:
    - label: `hypershift.openshift.io/tuned-config` : `true`
    - annotation: `hypershift.openshift.io/nodePool` : `NodePool` API name where the `PeformanceProfile` which generate this `tuned` was referenced.
- This will trigger the reconcile operation in NTO for these kind of objects.

### MachineConfig

The proposal for this output object is to use the way NTO has already put in place to handle its own `MachineConfig` objects as it is.

- Once Performance Profile Controller has created the `MachineConfig` object as usual, it will embeded the object into a `configmap` in the `hosted-control-plane-namespace`.
- This `configmap` will have:
  - label: `hypershift.openshift.io/nto-generated-machine-config` : `true`
  - label: `hypershift.openshift.io/nodePool` : `NodePool` API name where the `PeformanceProfile` which generate this `MachineConfig` was referenced.
- This will trigger the reconcile operation in NodePool Controller for these objects.

### MCO KubeletConfig

__WIP__

Being this object also handled by MachineConfig Operator (MCO) as `MachineConfig`s the proposal is to handle them in a similar way.

- Once Performance Profile Controller has created the `Kubeletconfig` object as usual, it will embeded the object into a `configmap` in the `hosted-control-plane-namespace`.
- This `configmap` will have:
  - label: `hypershift.openshift.io/nto-generated-kubelet-config` : `true`
  - label: `hypershift.openshift.io/nodePool` : `NodePool` API name where the `PeformanceProfile` which generate this `MachineConfig` was referenced.
- This will trigger the reconcile operation in NodePool Controller for these objects.
  - This operation is __NOT__ in place already, so a modification in the NodePool Controller will be needed, but it would be quite similar to the one handling `MachineConfigs` right now.

> __development NOTE__: I am not sure this is the proper way to handle this because I still have some doubts regarding the way NodePool Controller handles `MachineConfig` objects, that's why I have asked @dagrayvid about that
>
> Here is the question:
>
> - Regarding NTO generated machine configs. I have seen that nodepool controller extract all the machine configurations from the configmaps,
>  then comprress them and store them into a core.v1.Secret in the hosted-control-plane-namespace ... and I do not understand this operation.
>  I mean, in the end machineconfig operator should handle this machine configs to apply them in the hosted cluster, right?
>  I thought there have been some adaptations in the MCO for hypershift to handle this, but I cannot find anything in the repo so any help will be really appreciated.

### RuntimeClass

__WIP__

The proposal is to handle these objects like NTO handles its `tuned` configurations once they are in the `hosted-control-plane-namespace`, that is synchronize them directly with the ones in the hosted cluster using the proper KubeConfig.
